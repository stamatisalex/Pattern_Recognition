# -*- coding: utf-8 -*-
"""2η Εργαστηριακή.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TnJkNWyDgqq2LN0OOrCDYVXGFe2FsCTV

#Δεύτερη εργαστηριακή άσκηση
* Αλεξανδρόπουλος Σταμάτης (03117060) 
* Γκότση Πολυτίμη-Άννα (03117201)

##Imports
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pomegranate

import numpy as np
import sklearn
import matplotlib.pyplot as plt
import librosa
import sys
#sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/Αναγνώριση Προτύπων/2ο Εργαστήριο/lab2')
sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/pattern_rec/2ο Εργαστήριο/lab2')
#import parser
#import hmm, lstm, plot_confusion_matrix
import torch
import random
import pandas as pd
import seaborn as sns
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
import statistics
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn import svm
from sklearn import preprocessing
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch import optim
import torch.utils.data as data_utils
import re
import librosa
import os
from glob import glob
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm
import matplotlib.pyplot as plt
import librosa.display
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from scipy.stats import multivariate_normal
from mpl_toolkits.axes_grid1 import make_axes_locatable
import itertools

!pip install gensim==3.8.1 matplotlib==3.1.0 nltk==3.4.4 numpy==1.16.4 pandas==0.24.2 pomegranate==0.12.0 scikit-image==0.15.0 scikit-learn==0.21.2 scipy==1.3.0 seaborn==0.9.0 torch==1.3.1 torchvision==0.4.2 tqdm==4.32.1 joblib==0.17.0
!pip install numba==0.48.0 --ignore-installed
!pip install librosa==0.7.1

"""## Bήμα 2: Read sound files and get label and speaker numbers

"""

def data_parser(directory):                                      # function to read sound files and return three lists: sound list, label list, speaker number list
    # Parse relevant dataset info
    files = glob(os.path.join(directory, "*.wav"))
    fnames = [f.split("/")[9].split(".")[0] for f in files]
    temp = re.compile("([a-zA-Z]+)([0-9]+)")
    fnames = [temp.match(f).groups() for f in fnames]
    digits = [f[0] for f in fnames]
    speakers = [f[1] for f in fnames]
    _, Fs = librosa.core.load(files[0], sr=None)                # get sampling frequency

    def read_wav(f):
        wav, _ = librosa.core.load(f, sr=None)                  # load wav file

        return wav
   
    wavs = [read_wav(f) for f in files]                         # read all wavs
    print("Total wavs: {}. Fs = {} Hz".format(len(wavs), Fs))   # print dataset info

    return wavs, digits, speakers

wavs,digits,speakers=data_parser('/content/drive/MyDrive/Colab Notebooks/pattern_rec/2ο Εργαστήριο/pr_lab2_2020-21_data/digits')  # read sound files and get three lists: sound list, label list, speaker number list
#wavs,digits,speakers=data_parser('/content/drive/MyDrive/Colab Notebooks/Αναγνώριση Προτύπων/2ο Εργαστήριο/pr_lab2_2020-21_data/digits')

"""Αντιστοιχούμε τις ετικέτες κάθε δείγματος από λέξεις σε νούμερα για μεγαλύτερη ευκολία:"""

units = ['one','two','three','four','five','six','seven','eight','nine']
l=dict(enumerate(units,start=1))
inv_map = {v: k for k, v in l.items()}
digits=[inv_map[i] for i in digits]

"""##Βήμα 3: MFCCs, Deltas, Delta-deltas

"""

def extract_features(wavs, n_mfcc=13, Fs=16000):  # extract MFCCs for all wavs, returns sample_number*frame_number*n_mfcc array
    window = 25 * Fs // 1000                      # window size
    step = window // 2                            # step size
    hop_length=10*Fs//1000                        # number of samples between successive frames
    frames = [                                    # extract MFCCs for all frames for each utterance
        #librosa.feature.mfcc(wav, Fs, n_fft=window, hop_length=window - step, n_mfcc=n_mfcc).T
        librosa.feature.mfcc(wav, Fs, n_fft=window, hop_length=hop_length, n_mfcc=n_mfcc).T  # extract n_fft MFCCs
        for wav in tqdm(wavs, desc="Extracting mfcc features...")
    ]

    print("Feature extraction completed with {} mfccs per frame".format(n_mfcc))
    return frames

frames=extract_features(wavs)                   # get MFCCs for all frames for all utterances

print('MFCCs array 1st dimension size:',len(frames))
print('MFCCs array2nd dimension size:',len(frames[0]))
print('MFCCs array3rd dimension size:',len(frames[0][0]))

def calculate_deltas(frames):                                                # function to calculate local derivatives and second local derivatives of features
  mfcc_deltas = [librosa.feature.delta(mfcc) for mfcc in frames]             #  first local derivatives
  mfcc_delta2 = [librosa.feature.delta(mfcc, order=2) for mfcc in frames]    #  second local derivatives
  return mfcc_deltas,mfcc_delta2                                             # returns two arrays, with size same as frames

deltas1,deltas2=calculate_deltas(frames)                                     # calculate deltas and delta-deltas for features

print('deltas 1st dimension size:',len(deltas1))
print('deltas 2nd dimension size:',len(deltas1[0]))
print('deltas 3rd dimension size:',len(deltas1[0][0]))
print('delta-deltas 1st dimension size:',len(deltas2))
print('delta-deltas 2nd dimension size:',len(deltas2[0]))
print('delta-deltas 3rd dimension size:',len(deltas2[0][0]))

"""##Βήμα 4: MFCCs histograms, MFSCs, MFSCs and MFCCs comparison

###Ιστογράμματα ανά utterance:

####Ψηφίο "one":
"""

# calculate histograms for every utterance of digit 1:

#count number of utterances for this digit:
num=0
for i in range(0,len(frames)):                                  
  if (digits[i]==1):
    num+=1

fig,ax=plt.subplots(num,4,figsize=(26,6*num))   

j=0
for i in range(0,len(frames)):                                  
  if (digits[i]==1):

    #1st MFCC
    ax[j][0].hist(frames[i][:,0])
    ax[j][1].hist(frames[i][:,0], density=True)
    ax[j][0].set_ylabel('Frequency')
    ax[j][1].set_ylabel('Probability')
    ax[j][0].set_xlabel('Feature value')
    ax[j][1].set_xlabel('Feature value')
    ax[j][0].set_title('First MFCC for utterance {}, digit 1, speaker {}'.format(i,speakers[i]))
    ax[j][1].set_title('First MFCC for utterance {}, digit 1, speaker {}, normalized'.format(i,speakers[i]))

    #2nd MFCC
    ax[j][2].hist(frames[i][:,1])
    ax[j][3].hist(frames[i][:,1], density=True)
    ax[j][2].set_ylabel('Frequency')
    ax[j][3].set_ylabel('Probability')
    ax[j][2].set_xlabel('Feature value')
    ax[j][3].set_xlabel('Feature value')
    ax[j][2].set_title('Second MFCC for utterance {}, digit 1, speaker {}'.format(i,speakers[i]))
    ax[j][3].set_title('Second MFCC for utterance {}, digit 1, speaker {}, normalized'.format(i,speakers[i]))
    
    j+=1

plt.show()

"""####Ψηφίο "six":"""

# calculate histograms for every utterance of digit 6:

#count number of utterances for this digit:
num=0
for i in range(0,len(frames)):                                  
  if (digits[i]==6):
    num+=1

fig,ax=plt.subplots(num,4,figsize=(26,6*num))   

j=0
for i in range(0,len(frames)):                                  
  if (digits[i]==6):

    #1st MFCC
    ax[j][0].hist(frames[i][:,0])
    ax[j][1].hist(frames[i][:,0], density=True)
    ax[j][0].set_ylabel('Frequency')
    ax[j][1].set_ylabel('Probability')
    ax[j][0].set_xlabel('Feature value')
    ax[j][1].set_xlabel('Feature value')
    ax[j][0].set_title('First MFCC for utterance {}, digit 6, speaker {}'.format(i,speakers[i]))
    ax[j][1].set_title('First MFCC for utterance {}, digit 6, speaker {}, normalized'.format(i,speakers[i]))

    #2nd MFCC
    ax[j][2].hist(frames[i][:,1])
    ax[j][3].hist(frames[i][:,1], density=True)
    ax[j][2].set_ylabel('Frequency')
    ax[j][3].set_ylabel('Probability')
    ax[j][2].set_xlabel('Feature value')
    ax[j][3].set_xlabel('Feature value')
    ax[j][2].set_title('Second MFCC for utterance {}, digit 6, speaker {}'.format(i,speakers[i]))
    ax[j][3].set_title('Second MFCC for utterance {}, digit 6, speaker {}, normalized'.format(i,speakers[i]))
    
    j+=1

plt.show()

"""###Ιστογράμματα για όλα τα utterances μαζί"""

# calculate histograms for digit 1:

digits_one=[]                                               # array to hold MFCCs for each sample of digit 1
for i in range(0,len(frames)):                                  
  if (digits[i]==1):
    digits_one.append(np.mean(frames[i], axis=0))           # for each sample of digit 1 add mean MFCC from all windows
digits_one=np.array(digits_one)

fig,(ax1,ax2,ax3,ax4)=plt.subplots(1,4,figsize=(26,6)) 
  

#1st MFCC
ax1.hist(digits_one[:,0])
ax2.hist(digits_one[:,0], density=True)
ax1.set_ylabel('Frequency')
ax2.set_ylabel('Probability')
ax1.set_xlabel('Feature value')
ax2.set_xlabel('Feature value')
ax1.set_title('First MFCC for digit 1')
ax2.set_title('First MFCC for digit 1, normalized')

#2nd MFCC
ax3.hist(digits_one[:,1])
ax4.hist(digits_one[:,1], density=True)
ax3.set_ylabel('Frequency')
ax4.set_ylabel('Probability')
ax3.set_xlabel('Feature value')
ax4.set_xlabel('Feature value')
ax3.set_title('Second MFCC for digit 1')
ax4.set_title('Second MFCC for digit 1, normalized')

plt.show()

# calculate histograms for digit 6:

digits_six=[]                                               # array to hold MFCCs for each sample of digit 6
for i in range(0,len(frames)):                                  
  if (digits[i]==6):
    digits_six.append(np.mean(frames[i], axis=0))           # for each sample of digit 6 add mean MFCC from all windows
digits_six=np.array(digits_six)

fig,(ax1,ax2,ax3,ax4)=plt.subplots(1,4,figsize=(26,6)) 
  

#1st MFCC
ax1.hist(digits_six[:,0])
ax2.hist(digits_six[:,0], density=True)
ax1.set_ylabel('Frequency')
ax2.set_ylabel('Probability')
ax1.set_xlabel('Feature value')
ax2.set_xlabel('Feature value')
ax1.set_title('First MFCC for digit 6')
ax2.set_title('First MFCC for digit 6, normalized')

#2nd MFCC
ax3.hist(digits_six[:,1])
ax4.hist(digits_six[:,1], density=True)
ax3.set_ylabel('Frequency')
ax4.set_ylabel('Probability')
ax3.set_xlabel('Feature value')
ax4.set_xlabel('Feature value')
ax3.set_title('Second MFCC for digit 6')
ax4.set_title('Second MFCC for digit 6, normalized')

plt.show()

"""###Εξαγωγή MFSCs"""

def extract_MFSCs(wav, n=13, Fs=16000):   # extract MFCCs for given wav
    window = 25 * Fs // 1000              # window size
    step = window // 2                    # step size
    hop_length=10*Fs//1000                # number of samples between successive frames
    frame = librosa.feature.melspectrogram(wav, Fs, n_fft=window, hop_length=hop_length, n_mels=n).T   # extract n_fft MFSCs
    print("MFSC extraction completed with {} features per frame".format(n))
    return frame

# choose two random utterances for each of digits one and six:
while True:                 # choose two random utterances (from different speakers):
  sample1,sample2=random.sample([i for i,value in enumerate(digits) if value==1],2)  # pick the indexes of two samples that are digit one
  if speakers[sample1]!=speakers[sample2]:                                               # pick again if samples piched belong to the same speaker
    break
while True:                 # choose two random utterances (from different speakers):
  sample3,sample4=random.sample([i for i,value in enumerate(digits) if value==6],2)  # pick the indexes of two samples that are digit one
  if speakers[sample3]!=speakers[sample4]:                                               # pick again if samples piched belong to the same speaker
    break
print('Speakers for digit one:', speakers[sample1], 'and', speakers[sample2])
print('Speakers for digit six:', speakers[sample3], 'and', speakers[sample4])

# extract MFSCs for chosen utterances: 
MFSC_one_1=extract_MFSCs(wavs[sample1])
MFSC_one_2=extract_MFSCs(wavs[sample2])
MFSC_six_1=extract_MFSCs(wavs[sample3])
MFSC_six_2=extract_MFSCs(wavs[sample4])

"""###Διαγράμματα Συσχέτισης

Ένας τρόπος για την αναπαράσταση του διαγράμματος συσχέτισης είναι ο εξής:
"""

# show correlation matrix using seaborn:
fig = plt.figure(figsize = (10, 15))
ax1 = fig.add_subplot(4, 2, 1)
ax1 = sns.heatmap (np.corrcoef(MFSC_one_1, rowvar = False))
ax1.set_title ('MFSC correlation matrix: Speaker '+str(speakers[sample1]) + ', Digit 1', fontsize=13, pad=18)
plt.show ()

"""Στην συνέχεια χρησιμοποιούμε όμως έναν δεύτερο τρόπο:"""

def plot_correlation(mfsc, mfcc, title):      # function to plot correlation matrices for MFSCs and MFCCs
    df1 = pd.DataFrame(data=mfsc, index=range(mfsc.shape[0]), columns=range(mfsc.shape[1]))  # convert MFSC array to panda dataframe
    df2 = pd.DataFrame(data=mfcc, index=range(mfcc.shape[0]), columns=range(mfcc.shape[1]))  # convert MFCC array to panda dataframe
    fig,(ax1,ax2)=plt.subplots(1,2, figsize=(10,6))
    
    # MFSC correlation matrix
    im1=ax1.matshow(df1.corr())
    divider = make_axes_locatable(ax1)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im1, cax=cax, orientation='vertical')
    ax1.set_title("MFSCs", pad=30)

    # MFCC correlation matrix
    im2=ax2.matshow(df2.corr())
    divider = make_axes_locatable(ax2)
    cax = divider.append_axes('right', size='5%', pad=0.05)
    fig.colorbar(im2, cax=cax, orientation='vertical')
    ax2.set_title("MFCCs", pad=30)

    fig.suptitle(title,fontsize=15)
    plt.show()

plot_correlation(MFSC_one_1, frames[sample1], "Correlation Matrix of uterrance {} of digit 1".format(sample1))

plot_correlation(MFSC_one_2, frames[sample2], "Correlation Matrix of uterrance {} of digit 1".format(sample2))

plot_correlation(MFSC_six_1, frames[sample3], "Correlation Matrix of uterrance {} of digit 6".format(sample3))

plot_correlation(MFSC_six_2, frames[sample4], "Correlation Matrix of uterrance {} of digit 6".format(sample1))

"""##Βήμα 5: Unique feature vector extraction and first two features scatter plot"""

def calculate_unique_vector(deltas,deltadeltas,mfccs):  # returns a nxd array, where n the number of utterances, d=6x13  (13 features for each of mfccs, deltas, deltadeltas, one mean and one std for each of the features)
  features=[]                                                   # nxd array (n unique vectors of dimension d)
  for i in range(0,len(mfccs)):
    sample_features=[]                                          # 1D array to hold 6x13 features for utterance i
    for feature in range(0,len(mfccs[0][0])):                   # for each of the 13 features
      mean_mfcc=np.mean(mfccs[i][:,feature])                    # calculate mean
      mean_delta=np.mean(deltas[i][:,feature])                  # calculate std
      mean_deltadelta=np.mean(deltadeltas[i][:,feature])
      std_mfcc=np.std(mfccs[i][:,feature])
      std_delta=np.std(deltas[i][:,feature])
      std_deltadelta=np.std(deltadeltas[i][:,feature])
      sample_features.extend([mean_mfcc,std_mfcc,mean_delta,std_delta,mean_deltadelta,std_deltadelta])   # add all means and variances to feature array for this utterance
    features.append(sample_features)                            # append array of features for utterance i
  return features

unique_vectors=np.array(calculate_unique_vector(deltas1,deltas2,frames))
print('Feature array has shape:',unique_vectors.shape)

"""Αναπαριστούμε τις δύο πρώτες διαστάσεις των διανυσμάτων χαρακτηριστικών σε scatter plot:"""

X, Y = unique_vectors[:,0], unique_vectors[:,1]  # x is first dimension, y is second
label_names=['one','two','three','four','five','six','seven','eight','nine','ten']
colors = ['brown', 'red', 'yellow', 'blue', 'green', 'black', 'purple', 'orange', 'silver', 'grey']
fig, ax = plt.subplots(figsize=(7,5))
for i in range(0,len(set(digits))):
    ax.scatter(X[np.array(digits) == i+1], Y[np.array(digits) == i+1],c=(colors[i]), label=(label_names[i]),s=60, alpha=0.9, edgecolors='k')
ax.legend()  
ax.grid()  
ax.set_xlabel('Feature 1')
ax.set_ylabel('Feature 2')
ax.set_title('Scatter plot of first two features for dataset',fontsize=17)
plt.show()

"""##Βήμα 6: Scatter plots with PCA

###Τwo principal components
"""

pca = PCA(n_components=2)                                                                                                               # we will use PCA to reduce dimensionality (78 --> 2 features)
pca_vectors  = pca.fit_transform(unique_vectors)                                                                                        # adapt data to reduced demensionality keeping 2 principal components (2 features)
print("First component contains {:.2f}% of the total information.".format((100*pca.explained_variance_ratio_[0])))
print("Second component contains {:.2f}% of the total information.".format( (100*pca.explained_variance_ratio_[1])))
print("The 2 components combined contain {:.2f}% of the total information.".format((100*pca.explained_variance_ratio_[0])+(100*pca.explained_variance_ratio_[1])) )

X, Y = pca_vectors[:,0], pca_vectors[:,1]
label_names=['one','two','three','four','five','six','seven','eight','nine']
colors = ['brown', 'red', 'yellow', 'blue', 'green', 'black', 'purple', 'orange', 'silver']
fig, ax = plt.subplots(figsize=(7,5))
for i in range(0,len(set(digits))):
    ax.scatter(X[np.array(digits) == i+1], Y[np.array(digits) == i+1],c=(colors[i]), label=(label_names[i]),s=60, alpha=0.9, edgecolors='k')
ax.legend()    
ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_title('Scatter plot of first two Principal Components for dataset',fontsize=17)
ax.grid()
plt.show()

"""###Τhree principal components"""

pca = PCA(n_components=3)                                                                                                     # we will use PCA to reduce dimensionality (78 --> 3 features)
pca_vectors  = pca.fit_transform(unique_vectors)                                                                              # adapt data to reduced demensionality keeping 3 principal components (3 features)
print("First component contains {:.2f}% of the total information.".format((100*pca.explained_variance_ratio_[0])))
print("Second component contains {:.2f}% of the total information.".format( (100*pca.explained_variance_ratio_[1])))
print("Third component contains {:.2f}% of the total information.".format( (100*pca.explained_variance_ratio_[2])))
print("The 3 components combined contain {:.2f}% of the total information.".format((100*pca.explained_variance_ratio_[0])+(100*pca.explained_variance_ratio_[1])+(100*pca.explained_variance_ratio_[2])) )

X, Y ,Z = pca_vectors[:,0], pca_vectors[:,1], pca_vectors[:,2]
label_names=['one','two','three','four','five','six','seven','eight','nine']
colors = ['brown', 'red', 'yellow', 'blue', 'green', 'black', 'purple', 'orange', 'silver']
fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1,projection = '3d')
for i in range(0,len(set(digits))):
    ax.scatter(X[np.array(digits) == i+1], Y[np.array(digits) == i+1],Z[np.array(digits)==i+1], c=(colors[i]), label=(label_names[i]),alpha=0.9)
ax.set_xlabel('Principal Component 1')
ax.set_ylabel('Principal Component 2')
ax.set_zlabel('Principal Component 3')
ax.set_title("Scatter plot of first three Principal Components", fontsize=17)
ax.legend() 
ax.grid()
plt.tight_layout()

"""##Βήμα 7: Dataset split, Classification and Extra feature addition

###Χωρισμός δεδομένων σε train και test
"""

X_train,X_test,y_train,y_test=train_test_split(unique_vectors,np.array(digits),test_size=0.3)   # split data in 70% train, 20% test

y_train=np.array(y_train)  # convert label lists to numpy arrays
y_test=np.array(y_test)

print('Train set dimensions:',X_train.shape)
print('Train labels dimensions:',y_train.shape)
print('Test set dimensions:',X_test.shape)
print('Test labels dimensions:',y_test.shape)

"""###Χρήση ταξινομητών

####Κανονικοποίηση δεδομένων
"""

#standardization
scaler = preprocessing.StandardScaler().fit(X_train)  
X_train_standardized = scaler.transform(X_train)   # transform train data
X_test_standardized = scaler.transform(X_test)     # transform test data

#min max scaling
min_max_scaler = preprocessing.MinMaxScaler()
X_train_minmax = min_max_scaler.fit_transform(X_train)  # transform train data
X_test_minmax = min_max_scaler.transform(X_test)        # transform test data

"""####Custom Bayesian

#####Custom Bayes implementation
"""

def get_digit_indexes(y,digit):
    '''Takes a table of labels returns all indexes of the table where the label corresponds to a specific digit/class.

    Args:
        digit (int): The digit whose indexes we are searching.
        y (np.ndarray): Labels for dataset (nsamples)

    Returns:
        indexes (np.ndarray): A 1d table with the indexes found.
    '''
    indexes=[]
    for index,label in enumerate(y):        #loop through labels
        val=int(label)
        if val==digit:                      # if the label corresponds to the digit
            indexes.append(index)           # add the index to the table
    return indexes
def digit_mean_for_feature(X, y, digit, feature):
    '''Calculates the mean for all instances of a specific digit for a specific feature

    Args:
        X (np.ndarray): Digits data (nsamples x nfeatures)
        y (np.ndarray): Labels for dataset (nsamples)
        digit (int): The digit we need to select
        feature (int): The feature we need to select

    Returns:
        (float): The mean value of the digits for the specified feature
    '''
    indexes=get_digit_indexes(y,digit)                      # get all indexes for the specific digit in the data
    return statistics.mean(X[indexes,feature])              # return mean value for the specific pixel/feature
def digit_variance_for_feature(X, y, digit, feature):
    '''Calculates the variance for all instances of a specific digit for a specific feature

    Args:
        X (np.ndarray): Digits data (nsamples x nfeatures)
        y (np.ndarray): Labels for dataset (nsamples)
        digit (int): The digit we need to select
        feature (int): The feature we need to select

    Returns:
        (float): The variance value of the digits for the specified feature
    '''
    indexes=get_digit_indexes(y,digit)        # get all indexes for the specific digit in the data
    return np.var(X[indexes,feature])         # return variance for the specific feature

def digit_mean(X, y, digit):
    '''Calculates the mean for all instances of a specific digit

    Args:
        X (np.ndarray): Digits data (nsamples x nfeatures)
        y (np.ndarray): Labels for dataset (nsamples)
        digit (int): The digit we need to select

    Returns:
        (np.ndarray): The mean value of the digits for every feature
    '''
    mean_values=np.zeros(X.shape[1])                       # table where the mean for all instances of a specific digit will be kept
    
    for i in range(0,X.shape[1]):                          # calculate mean value for all features
      mean_values[i]=digit_mean_for_feature(X,y,digit,i) 
    return mean_values

def digit_variance(X, y, digit):
    '''Calculates the variance for all instances of a specific digit

    Args:
        X (np.ndarray): Digits data (nsamples x nfeatures)
        y (np.ndarray): Labels for dataset (nsamples)
        digit (int): The digit we need to select

    Returns:
        (np.ndarray): The variance value of the digits for every feature
    '''
    variance_values=np.zeros(X.shape[1])                # table where the variance for all instances of a specific digit will be kept
    for i in range (0,X.shape[1]):                       # calculate variance for all features  
      variance_values[i]=digit_variance_for_feature(X,y,digit,i)
    return variance_values

def calculate_priors(X, y):
    """Return the a-priori probabilities for every class

    Args:
        X (np.ndarray): Digits data (nsamples x nfeatures)
        y (np.ndarray): Labels for dataset (nsamples)

    Returns:
        (np.ndarray): (n_classes) Prior probabilities for every class
    """
    samples=X.shape[0]                                         # get number of samples
    element, occurence=np.unique(y,return_counts=True)         # calculate how often each label occurs
    priors=np.zeros(len(set(y)))                               # array that will hold a-priori probability for each class
    for i,times in enumerate(occurence):
      priors[i] = times/samples                                # calculate probability for each label
    return priors

class CustomNBClassifier(BaseEstimator, ClassifierMixin):
    """Custom implementation Naive Bayes classifier"""

    def __init__(self, use_unit_variance=False, smoothing=0.00001):
        self.use_unit_variance = use_unit_variance
        self.mean_=None
        self.covmatrix_=None
        self.priors_=None
        self.smoothing=smoothing


    def fit(self, X, y):
        """
        This should fit classifier. All the "work" should be done here.

        Calculates self.X_mean_ based on the mean
        feature values in X for each class.

        self.X_mean_ becomes a numpy.ndarray of shape
        (n_classes, n_features)

        fit always returns self.
        """
        num_of_classes=len(set(y))                                          # number of distinct labels
        smoothing = self.smoothing                                          # smoothing parameter
        mymean = np.zeros((num_of_classes,X.shape[1]))                      # table to hold mean values for features of each class
        mycovmatrix = np.zeros((num_of_classes,X.shape[1],X.shape[1]))      # covariance table

        if (not self.use_unit_variance):
            mycov = np.zeros((num_of_classes,X.shape[1]))                   # table to hold variance values for features of each class
            for class_ in range(1,num_of_classes+1):
                mymean[class_-1,:] = digit_mean(X,y,class_)                 # calculate mean values of characteristics for each class
                mycov[class_-1,:] = digit_variance(X,y,class_)              # calculate variance values of characteristics for each class
        else:
            for class_ in range(1,num_of_classes+1):
                mymean[class_-1,:] = digit_mean(X,y,class_)                 # calculate mean values of characteristics for each class
            mycov = np.ones((num_of_classes,X.shape[1]))                    # set variance value equal to one for all features

        for class_ in range(num_of_classes):
            for feature in range(X.shape[1]):
                mycovmatrix[class_,feature,feature] = mycov[class_,feature] + smoothing    # diagonal elements (variances) are added and smoothing is added too, all other covariances are zero (uncorrelated features)

        self.mean_=mymean
        self.covmatrix_=mycovmatrix
        self.priors_=calculate_priors(X, y)

        return self


    def predict(self, X):
        """
        Make predictions for X based on the
        multivariance
        """
        multipdfs = np.zeros((self.mean_.shape[0],X.shape[0]))
        for i in range(self.mean_.shape[0]):
            multipdfs[i,:] = multivariate_normal.logpdf(X, mean = self.mean_[i], cov = self.covmatrix_[i])
            multipdfs[i,:] += np.log(self.priors_[i])
        return np.argmax(multipdfs,axis = 0)+1

    def score(self, X, y):
        """
        Return accuracy score on the predictions
        for X based on ground truth y
        """
        predictions=self.predict(X)                 # get classifier's predictions
        return np.sum(predictions==y)/len(y)        # calculate percantage of correct ones

"""#####Custom Bayes

######Standardization:

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'smoothing': [0.0,0.00000001,0.0000001, 0.0000005, 0.0000008, 0.000001,0.000003, 0.000005, 0.000008, 0.00001, 0.00005, 0.0001]}                   # options for smoothing parameter for Naive Bayes classifier
grid = GridSearchCV(CustomNBClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_standardized, y_train)                                                     # run fit with all sets of parameters

print("Best parameter for Naive Bayes Classifier is : " +str(grid.best_params_))            # print best set of parameters

"""Αξιολογούμε την επίδοση του ταξινομητή μας:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(CustomNBClassifier(0.0),X_train_standardized,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=CustomNBClassifier(0.0)
clf.fit(X_train_standardized,y_train)
score=clf.score(X_test_standardized,y_test)
print("Accuracy on test data:",score)

"""######ΜinMax scaling

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'smoothing': [0.0,0.00000001,0.0000001, 0.0000005, 0.0000008, 0.000001,0.000003, 0.000005, 0.000008, 0.00001, 0.00005, 0.0001]}                   # options for smoothing parameter for Naive Bayes classifier
grid = GridSearchCV(CustomNBClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_minmax, y_train)                                                           # run fit with all sets of parameters

print("Best parameter for Naive Bayes Classifier is : " +str(grid.best_params_))            # print best set of parameters

"""Αξιολογούμε την επίδοση του ταξινομητή μας:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(CustomNBClassifier(0.0),X_train_minmax,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=CustomNBClassifier(0.0)
clf.fit(X_train_minmax,y_train)
score=clf.score(X_test_minmax,y_test)
print("Accuracy on test data:",score)

"""####Naive Bayes

######Standardization

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'var_smoothing': [0.0,0.00000001,0.0000001, 0.0000005, 0.0000008, 0.000001,0.000003, 0.000005, 0.000008, 0.00001, 0.00005, 0.0001]}      # options for smoothing parameter for Naive Bayes classifier
grid = GridSearchCV(GaussianNB(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_standardized, y_train)                                             # run fit with all sets of parameters

print("Best parameter for Naive Bayes Classifier is : " +str(grid.best_params_))    # print best set of parameters

"""Αξιολογούμε την επίδοση του ταξινομητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(GaussianNB(var_smoothing=0.0),X_train_standardized,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=GaussianNB(var_smoothing=0.0)
clf.fit(X_train_standardized,y_train)
score=clf.score(X_test_standardized,y_test)
print("Accuracy on test data:",score)

"""######MinMax scaling

Πραγματοποιούμε hyperparameter tuning
"""

param_grid = {'var_smoothing': [0.0,0.00000001,0.0000001, 0.0000005, 0.0000008, 0.000001,0.000003, 0.000005, 0.000008, 0.00001, 0.00005, 0.0001]}     # options for smoothing parameter for Naive Bayes classifier
grid = GridSearchCV(GaussianNB(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_minmax, y_train)                                                   # run fit with all sets of parameters

print("Best parameter for Naive Bayes Classifier is : " +str(grid.best_params_))    # print best set of parameters

"""Αξιολογούμε την επίδοση του εκτιμητή"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(GaussianNB(var_smoothing=0.0),X_train_minmax,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=GaussianNB(var_smoothing=0.0)
clf.fit(X_train_minmax,y_train)
score=clf.score(X_test_minmax,y_test)
print("Accuracy on test data:",score)

"""####kNN

######Standardization

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'n_neighbors':[2,3,4,5,6,7,8], 'weights': ['uniform', 'distance'],'algorithm':['auto', 'ball_tree', 'kd_tree']}   # options for parameters for KNN classifier
grid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_standardized, y_train)                                                       # run fit with all sets of parameters

print("Best parameters for Nearest Neighbors Classifier are : " +str(grid.best_params_))      # print best set of parameters

"""Αξιολογούμε την επίδοση του ταξινομητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(KNeighborsClassifier(n_neighbors=2,weights='distance',algorithm='auto'),X_train_standardized,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=KNeighborsClassifier(n_neighbors=2,weights='distance',algorithm='auto')
clf.fit(X_train_standardized,y_train)
score=clf.score(X_test_standardized,y_test)
print("Accuracy on test data:",score)

"""######MinMax scaling

Πραγματοποιούμε hyperparameter tuning
"""

param_grid = {'n_neighbors':[2,3,4,5,6,7,8], 'weights': ['uniform', 'distance'],'algorithm':['auto', 'ball_tree', 'kd_tree']}   # options for parameters for KNN classifier
grid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_minmax, y_train)                                                             # run fit with all sets of parameters

print("Best parameters for Nearest Neighbors Classifier are : " +str(grid.best_params_))      # print best set of parameters

"""Αξιολογούμε την επίδοση του εκτιμητή"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(KNeighborsClassifier(n_neighbors=2,weights='distance',algorithm='auto'),X_train_minmax,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=KNeighborsClassifier(n_neighbors=2,weights='distance',algorithm='auto')
clf.fit(X_train_minmax,y_train)
score=clf.score(X_test_minmax,y_test)
print("Accuracy on test data:",score)

"""####SVM

######Standardization

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'C': [0.000000001, 0.000000005, 0.00000001, 0.00000005, 0.0000001, 0.0000005, 0.000001, 0.000005, 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005,0.01,0.03,0.05],'gamma':['scale','auto'],'kernel': ['linear','rbf', 'poly','sigmoid']}  # options for parameters for  SVM classifier
  
grid = GridSearchCV(svm.SVC(), param_grid, refit = True,  cv = 5, n_jobs = -1)            # initialize GridSearchCV instance
grid.fit(X_train_standardized, y_train)                                                   # run fit with all sets of parameters

print("Best parameters for SVM model are : " +str(grid.best_params_))                    # print best set of parameters

"""Αξιολογούμε την επίδοση του ταξινομητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(svm.SVC(C= 0.03, gamma= 'scale', kernel= 'linear'),X_train_standardized,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=svm.SVC(C= 0.03, gamma= 'scale', kernel= 'linear')
clf.fit(X_train_standardized,y_train)
score=clf.score(X_test_standardized,y_test)
print("Accuracy on test data:",score)

"""######MinMax scaling

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'C': [0.0000001, 0.0000005, 0.000001, 0.000005, 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005,0.01,0.03,0.05],'gamma':['scale','auto'],'kernel': ['linear','rbf', 'poly','sigmoid']}  # options for parameters for  SVM classifier
  
grid = GridSearchCV(svm.SVC(), param_grid, refit = True,  cv = 5, n_jobs = -1)           # initialize GridSearchCV instance
grid.fit(X_train_minmax, y_train)                                                        # run fit with all sets of parameters

print("Best parameters for SVM model are : " +str(grid.best_params_))                    # print best set of parameters

"""Αξιολογούμε την επίδοση του εκτιμητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(svm.SVC(C= 0.03, gamma= 'scale', kernel= 'poly'),X_train_minmax,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=svm.SVC(C= 0.03, gamma= 'scale', kernel= 'poly')
clf.fit(X_train_minmax,y_train)
score=clf.score(X_test_minmax,y_test)
print("Accuracy on test data:",score)

"""####MLP

######Standardization

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'activation': ['identity', 'logistic', 'tanh', 'relu'],'solver':['lbfgs','sgd','adam'],'alpha': [0.00001,0.00005,0.0001,0.0005,0.01,0.05], 'learning_rate': ['constant', 'invscaling', 'adaptive']}  # options for parameters for  SVM classifier
  
grid = GridSearchCV(MLPClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)     # initialize GridSearchCV instance
grid.fit(X_train_standardized, y_train)                                                  # run fit with all sets of parameters

print("Best parameters for MLP model are : " +str(grid.best_params_))                    # print best set of parameters

"""Αξιλογούμε την επίδοση του ταξινομητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(MLPClassifier(activation='tanh',alpha=0.0005, learning_rate='constant',solver='lbfgs'),X_train_standardized,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=MLPClassifier(activation='tanh',alpha=0.0005, learning_rate='constant',solver='lbfgs')
clf.fit(X_train_standardized,y_train)
score=clf.score(X_test_standardized,y_test)
print("Accuracy on test data:",score)

"""######MinMax scaling

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'activation': ['identity', 'logistic', 'tanh', 'relu'],'solver':['lbfgs','sgd','adam'],'alpha': [0.00001,0.00005,0.0001,0.0005,0.01,0.05], 'learning_rate': ['constant', 'invscaling', 'adaptive']}  # options for parameters for  SVM classifier
  
grid = GridSearchCV(MLPClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)     # initialize GridSearchCV instance
grid.fit(X_train_minmax, y_train)                                                        # run fit with all sets of parameters

print("Best parameters for MLP model are : " +str(grid.best_params_))                    # print best set of parameters

"""Ελέγχουμε την επίδοση του ταξινομητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(MLPClassifier(activation='relu',alpha=0.0005, learning_rate='adaptive',solver='lbfgs'),X_train_minmax,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=MLPClassifier(activation='relu',alpha=0.0005, learning_rate='adaptive',solver='lbfgs')
clf.fit(X_train_minmax,y_train)
score=clf.score(X_test_minmax,y_test)
print("Accuracy on test data:",score)

"""###Bonus: Προσθήκη χαρακτηρηστικών

####Προσθήκη χαρακτηρηστικών, χωρισμός σε train-test και κανονικοποίηση νέων δεδομένων

Υπολογίζουμε τα χαρακτηρηστικά για κάθε εκφώνηση, προσθέτοντας τα νέα χαρακτηρηστικά:
"""

# calculate all features:

Fs=16000                   # sampling frequency
window = 25 * Fs // 1000   # window size
step = window // 2         # step size 
hop_length=10*Fs//1000     # number of samples between successive frames
n_mfcc=13                  # number of MFCCs to extract

# lists to keep extracted features:
mfccs=[]
deltas=[]
deltadeltas=[]
lpcss=[]
zero_crosses=[]
onset_envs=[]
chromas=[]
centroids=[]
rollofs=[]

for wav in wavs:

  #old features:
  mfcc= librosa.feature.mfcc(wav, Fs, n_fft=window, hop_length=hop_length, n_mfcc=n_mfcc).T
  delta=librosa.feature.delta(mfcc)
  deltadelta=librosa.feature.delta(mfcc,order=2)
  
  #added features:
  zero_cross=librosa.feature.zero_crossing_rate(wav,hop_length=hop_length)                    # zero crossing rate
  chroma=librosa.feature.chroma_stft(y=wav, sr=Fs,n_fft=window, hop_length=hop_length)        # chromogram
  S, phase = librosa.magphase(librosa.stft(wav))                                              # get madnitude of spectogram
  rollof=librosa.feature.spectral_rolloff(S=S, sr=Fs, n_fft=window, hop_length=hop_length)    # spectral rollof
  centroid=librosa.feature.spectral_centroid(wav,sr=Fs,n_fft=window, hop_length=hop_length)   # spectral centroid
  lpcs=librosa.lpc(wav,16)
  onset_env = librosa.onset.onset_strength(y=wav, sr=Fs)                                      # spectral flux
  
  #add all features to lists:
  mfccs.append(mfcc)
  deltas.append(delta)
  deltadeltas.append(deltadelta)
  lpcss.append(lpcs)
  zero_crosses.append(zero_cross)
  onset_envs.append(onset_env)
  chromas.append(chroma)
  centroids.append(centroid)
  rollofs.append(rollof)

# print(zero_crosses[0].shape)
# print(onset_envs[0].shape)
# print(lpcss[0].shape)
# print(chromas[0].shape)
# print(centroids[0].shape)
# print(rollofs[0].shape)

"""Εξάγουμε ένα μοναδικό διάνυσμα χαρακτηρηστικών για κάθε εκφώνηση:"""

#function to extract a feature vector for each utterance
def calculate_unique_vector_extra_features(mfccs,deltas,deltadeltas,lpcss,zero_crosses,onset_envs,chromas,centroids,rollofs):  # returns a nxd array, where n the number of utterances, d=6x13  (13 features for each of mfccs, deltas, deltadeltas, one mean and one std for each of the features)
  features=[]                                                   # nxd array (n unique vectors of dimension d)
  for i in range(0,len(mfccs)):
    sample_features=[]                                          # 1D array to hold features for utterance i
    for feature in range(0,len(mfccs[0][0])):                   # for each of the 13 features
      mean_mfcc=np.mean(mfccs[i][:,feature])                    # calculate mean
      mean_delta=np.mean(deltas[i][:,feature])                  # calculate std
      mean_deltadelta=np.mean(deltadeltas[i][:,feature])
      std_mfcc=np.std(mfccs[i][:,feature])
      std_delta=np.std(deltas[i][:,feature])
      std_deltadelta=np.std(deltadeltas[i][:,feature])
      sample_features.extend([mean_mfcc,std_mfcc,mean_delta,std_delta,mean_deltadelta,std_deltadelta])   # add means and variances to feature array for this utterance
    
    #calculate mean and std for all added features:
    mean_zero_cross=np.mean(zero_crosses[i])   
    std_zero_cross=np.std(zero_crosses[i])
    mean_lpcss=np.mean(lpcss[i])
    std_lpcss=np.std(lpcss[i])
    mean_onset_envs=np.mean(onset_envs[i])
    std_onset_envs=np.std(onset_envs[i])
    mean_chromas=np.mean(chromas[i])
    std_chromas=np.std(chromas[i])
    mean_centroids=np.mean(centroids[i])
    std_centroids=np.std(centroids[i])
    mean_rollof=np.mean(rollofs[i])
    std_rollof=np.mean(rollofs[i])
    
    #append means and stds of new features to features array:
    sample_features.extend([mean_zero_cross,std_zero_cross,mean_centroids,std_centroids,mean_rollof,std_rollof,mean_lpcss,std_lpcss,mean_onset_envs,std_onset_envs,mean_chromas,std_chromas])

    features.append(sample_features)                            # append array of features for utterance i
  return features

new_features=calculate_unique_vector_extra_features(mfccs,deltas,deltadeltas,lpcss,zero_crosses,onset_envs,chromas,centroids,rollofs)  # get new array of unique vectors

new_features=np.array([np.array(i) for i in new_features])   # convert to numpy array

"""Χωρίζουμε τα δεδομένα σε train και test:"""

#split data into train and test data:
X_train,X_test,y_train,y_test=train_test_split(new_features,digits,test_size=0.3,random_state=17)  # keep 70% for train and 30% for test
print('Train data array dimensions:',X_train.shape)
print('Test data array dimensions:',X_test.shape)

"""Κανονικοποιούμε τα δεδομένα:"""

#standardization
scaler = preprocessing.StandardScaler().fit(X_train)  
X_train_standardized = scaler.transform(X_train)   # transform train data
X_test_standardized = scaler.transform(X_test)     # transform test data

#min max scaling
min_max_scaler = preprocessing.MinMaxScaler()
X_train_minmax = min_max_scaler.fit_transform(X_train)
X_test_minmax = min_max_scaler.transform(X_test)

"""####Υπολογισμός επιδόσεων εκτιμητών

#####Custom Bayesian

######Standardization:

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'smoothing': [0.0,0.00000001,0.0000001, 0.0000005, 0.0000008, 0.000001,0.000003, 0.000005, 0.000008, 0.00001, 0.00005, 0.0001]}     # options for smoothing parameter for Naive Bayes classifier
grid = GridSearchCV(CustomNBClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_standardized, y_train)                                                     # run fit with all sets of parameters

print("Best parameter for Naive Bayes Classifier is : " +str(grid.best_params_))            # print best set of parameters

"""Αξιολογούμε την επίδοση του ταξινομητή μας:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(CustomNBClassifier(0.0),X_train_standardized,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=CustomNBClassifier(0.0)
clf.fit(X_train_standardized,y_train)
score=clf.score(X_test_standardized,y_test)
print("Accuracy on test data:",score)

"""######ΜinMax scaling

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'smoothing': [0.0,0.00000001,0.0000001, 0.0000005, 0.0000008, 0.000001,0.000003, 0.000005, 0.000008, 0.00001, 0.00005, 0.0001]}      # options for smoothing parameter for Naive Bayes classifier
grid = GridSearchCV(CustomNBClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_minmax, y_train)                                                           # run fit with all sets of parameters

print("Best parameter for Naive Bayes Classifier is : " +str(grid.best_params_))            # print best set of parameters

"""Αξιολογούμε την επίδοση του ταξινομητή μας:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(CustomNBClassifier(0.0),X_train_minmax,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=CustomNBClassifier(0.0)
clf.fit(X_train_minmax,y_train)
score=clf.score(X_test_minmax,y_test)
print("Accuracy on test data:",score)

"""#####Naive Bayes

######Standardization

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'var_smoothing': [0.0,0.00000001,0.0000001, 0.0000005, 0.0000008, 0.000001,0.000003, 0.000005, 0.000008, 0.00001, 0.00005, 0.0001]}     # options for smoothing parameter for Naive Bayes classifier
grid = GridSearchCV(GaussianNB(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_standardized, y_train)                                             # run fit with all sets of parameters

print("Best parameter for Naive Bayes Classifier is : " +str(grid.best_params_))    # print best set of parameters

"""Αξιολογούμε την επίδοση του ταξινομητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(GaussianNB(var_smoothing=0.0),X_train_standardized,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=GaussianNB(var_smoothing=0.0)
clf.fit(X_train_standardized,y_train)
score=clf.score(X_test_standardized,y_test)
print("Accuracy on test data:",score)

"""######MinMax scaling

Πραγματοποιούμε hyperparameter tuning
"""

param_grid = {'var_smoothing': [0.0,0.00000001,0.0000001, 0.0000005, 0.0000008, 0.000001,0.000003, 0.000005, 0.000008, 0.00001, 0.00005, 0.0001]}                   # options for smoothing parameter for Naive Bayes classifier
grid = GridSearchCV(GaussianNB(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_minmax, y_train)                                                   # run fit with all sets of parameters

print("Best parameter for Naive Bayes Classifier is : " +str(grid.best_params_))    # print best set of parameters

"""Αξιολογούμε την επίδοση του εκτιμητή"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(GaussianNB(var_smoothing=0.0),X_train_minmax,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=GaussianNB(var_smoothing=0.0)
clf.fit(X_train_minmax,y_train)
score=clf.score(X_test_minmax,y_test)
print("Accuracy on test data:",score)

"""#####kNN

######Standardization

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'n_neighbors':[2,3,4,5,6,7,8], 'weights': ['uniform', 'distance'],'algorithm':['auto', 'ball_tree', 'kd_tree']}   # options for parameters for KNN classifier
grid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_standardized, y_train)                                                       # run fit with all sets of parameters

print("Best parameters for Nearest Neighbors Classifier are : " +str(grid.best_params_))      # print best set of parameters

"""Αξιολογούμε την επίδοση του ταξινομητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(KNeighborsClassifier(n_neighbors=2,weights='distance',algorithm='auto'),X_train_standardized,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=KNeighborsClassifier(n_neighbors=2,weights='distance',algorithm='auto')
clf.fit(X_train_standardized,y_train)
score=clf.score(X_test_standardized,y_test)
print("Accuracy on test data:",score)

"""######MinMax scaling

Πραγματοποιούμε hyperparameter tuning
"""

param_grid = {'n_neighbors':[2,3,4,5,6,7,8], 'weights': ['uniform', 'distance'],'algorithm':['auto', 'ball_tree', 'kd_tree']}   # options for parameters for KNN classifier
grid = GridSearchCV(KNeighborsClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)   # initialize GridSearchCV instance
grid.fit(X_train_minmax, y_train)                                                             # run fit with all sets of parameters

print("Best parameters for Nearest Neighbors Classifier are : " +str(grid.best_params_))      # print best set of parameters

"""Αξιολογούμε την επίδοση του εκτιμητή"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(KNeighborsClassifier(n_neighbors=2,weights='distance',algorithm='auto'),X_train_minmax,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=KNeighborsClassifier(n_neighbors=2,weights='distance',algorithm='auto')
clf.fit(X_train_minmax,y_train)
score=clf.score(X_test_minmax,y_test)
print("Accuracy on test data:",score)

"""#####SVM

######Standardization

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'C': [0.000000001, 0.000000005, 0.00000001, 0.00000005, 0.0000001, 0.0000005, 0.000001, 0.000005, 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005,0.01,0.03,0.05],'gamma':['scale','auto'],'kernel': ['linear','rbf', 'poly','sigmoid']}  # options for parameters for  SVM classifier
  
grid = GridSearchCV(svm.SVC(), param_grid, refit = True,  cv = 5, n_jobs = -1)           # initialize GridSearchCV instance
grid.fit(X_train_standardized, y_train)                                                  # run fit with all sets of parameters

print("Best parameters for SVM model are : " +str(grid.best_params_))                    # print best set of parameters

"""Αξιολογούμε την επίδοση του ταξινομητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(svm.SVC(C= 0.03, gamma= 'scale', kernel= 'linear'),X_train_standardized,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=svm.SVC(C= 0.03, gamma= 'scale', kernel= 'linear')
clf.fit(X_train_standardized,y_train)
score=clf.score(X_test_standardized,y_test)
print("Accuracy on test data:",score)

"""######MinMax scaling

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'C': [0.0000001, 0.0000005, 0.000001, 0.000005, 0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005,0.01,0.03,0.05],'gamma':['scale','auto'],'kernel': ['linear','rbf', 'poly','sigmoid']}  # options for parameters for  SVM classifier
  
grid = GridSearchCV(svm.SVC(), param_grid, refit = True,  cv = 5, n_jobs = -1)           # initialize GridSearchCV instance
grid.fit(X_train_minmax, y_train)                                                        # run fit with all sets of parameters

print("Best parameters for SVM model are : " +str(grid.best_params_))                    # print best set of parameters

"""Αξιολογούμε την επίδοση του εκτιμητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(svm.SVC(C= 0.05, gamma= 'scale', kernel= 'poly'),X_train_minmax,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=svm.SVC(C= 0.05, gamma= 'scale', kernel= 'poly')
clf.fit(X_train_minmax,y_train)
score=clf.score(X_test_minmax,y_test)
print("Accuracy on test data:",score)

"""#####MLP

######Standardization

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'activation': ['identity', 'logistic', 'tanh', 'relu'],'solver':['lbfgs','sgd','adam'],'alpha': [0.00001,0.00005,0.0001,0.0005,0.01,0.05,0.08], 'learning_rate': ['constant', 'invscaling', 'adaptive']}  # options for parameters for  SVM classifier
  
grid = GridSearchCV(MLPClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)     # initialize GridSearchCV instance
grid.fit(X_train_standardized, y_train)                                                  # run fit with all sets of parameters

print("Best parameters for MLP model are : " +str(grid.best_params_))                    # print best set of parameters

"""Αξιλογούμε την επίδοση του ταξινομητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(MLPClassifier(activation='identity',alpha=0.01, learning_rate='invscaling',solver='lbfgs'),X_train_standardized,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=MLPClassifier(activation='identity',alpha=0.01, learning_rate='invscaling',solver='lbfgs')
clf.fit(X_train_standardized,y_train)
score=clf.score(X_test_standardized,y_test)
print("Accuracy on test data:",score)

"""######MinMax scaling

Πραγματοποιούμε hyperparameter tuning:
"""

param_grid = {'activation': ['identity', 'logistic', 'tanh', 'relu'],'solver':['lbfgs','sgd','adam'],'alpha': [0.00001,0.00005,0.0001,0.0005,0.01,0.05,0.08], 'learning_rate': ['constant', 'invscaling', 'adaptive']}  # options for parameters for  SVM classifier
  
grid = GridSearchCV(MLPClassifier(), param_grid, refit = True,  cv = 5, n_jobs = -1)            # initialize GridSearchCV instance
grid.fit(X_train_minmax, y_train)                                                                # run fit with all sets of parameters

print("Best parameters for MLP model are : " +str(grid.best_params_))                    # print best set of parameters

"""Ελέγχουμε την επίδοση του ταξινομητή:"""

#Evaluate with cross validation, on train data:
scores=cross_val_score(MLPClassifier(activation='identity',alpha=0.01, learning_rate='constant',solver='lbfgs'),X_train_minmax,y_train,cv=KFold(n_splits=5),scoring='accuracy')
print("Cross validation accuracy:",np.mean(scores))
#Evaluate on test data:
clf=MLPClassifier(activation='identity',alpha=0.01, learning_rate='constant',solver='lbfgs')
clf.fit(X_train_minmax,y_train)
score=clf.score(X_test_minmax,y_test)
print("Accuracy on test data:",score)

"""##Βήμα 8: RNN-LSTM-GRU

###Create dataset and split into train and test
"""

sine = np.zeros((1000, 10))                                   # array to keep samples
cosine = np.zeros((1000, 10))                                 # array to keep labels
f = 40                                                        # frequency of sine
step = 0.01
number_of_samples=1000                                        # number of samples (sine sequences)

for i in range(number_of_samples):
    A = np.random.rand()*10                                   # choose random amplitude
    begin = np.random.rand()*(40/f)                           # choose random starting point in range 0-40*T
    t = np.linspace(begin, begin+step, num=10)                # time axis
    sine[i] = A*np.sin(2*np.pi*f*t)                           # sine sequence (data)
    cosine[i] =  A*np.cos(2*np.pi*f*t)                        # cosine siquence (labels)

X_train, X_test, y_train, y_test = train_test_split(sine, cosine, test_size=0.2)   # split data into train and test

"""###RNN"""

n_iters = 3000      # number of iterations
batch_size = 500    # batch size

train = data_utils.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())   # create train dataset from train data and labels
train_loader = torch.utils.data.DataLoader(dataset=train,batch_size=batch_size,shuffle=True)             # create dataloader, shuffle data 

test = data_utils.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())      # create test dataset from test data and labels
test_loader = torch.utils.data.DataLoader(dataset=test,batch_size=len(X_test), shuffle=False)            # we dont want batches in test data, just a dataloader, so we set batch size equal to our test samples

num_epochs = int(n_iters / (len(X_train) / batch_size))                                                  # compute number of epochs from number of iterations we want, batch size and number of train samples  
print("Epochs for training:",num_epochs)
print("Batch size:",batch_size)

class RNNModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):
        super(RNNModel, self).__init__()
        self.hidden_dim = hidden_dim                                                                    # dimension of hidden layer
        self.layer_dim = layer_dim                                                                      # number of reccurent layers
        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')
        self.fc = nn.Linear(hidden_dim, output_dim)                                                     # linear layer

    def forward(self, x):

        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()                   # hidden state initialized with zeros 
        out, hn = self.rnn(x, h0.detach())                                                              # detach the hidden state to prevent exploding/vanishing gradients 
        out = self.fc(out[:, -1, :])                                                                    # keep only last time step of hidden state
        return out

input_dim = 1        # dimension of input
hidden_dim = 100     # hidden layer dimension 
layer_dim = 1        # number of hidden layers 
output_dim = 10      # dimemnsion of output 

model = RNNModel(input_dim, hidden_dim, layer_dim, output_dim)    # initialize output
criterion = nn.MSELoss()                                          # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)         # optimizer

# train model:
for j in range(num_epochs):
  for i, (data, labels) in enumerate(train_loader):
        optimizer.zero_grad()                            # set gradients to zero
        data=data.view(len(data),10,1)                   # addapt dimension to input to model   
        data=data.float()                                # convert tensor to to float tensor
        outputs = model(data)                            # give input to model and get output                    
        loss = criterion(outputs, labels)                # calculate loss 

        if j%100 == 0:                                   # print train loss every 100 epochs                           
          print('Train loss', loss.item())

        loss.backward()                                  # compute gradient   
        optimizer.step()                                 # update parameters

        if j%100 == 0:                                   # print test loss every 100 epochs
          for i, (data, labels) in enumerate(test_loader):
            with torch.no_grad():                        # disable gradient calculation as we will not call torch.backward()
              data=data.view(len(data),10,1)
              data=data.float()
              pred = model(data)
              loss = criterion(pred,labels)
              
              print('Test loss:', loss.item())
              print()

# plot sum of the sine sequences and the predicted cosine
for i, (data, labels) in enumerate(test_loader):   
    data_copy=data
    with torch.no_grad():                 # do not compute gradients
        data=data.view(len(data),10,1)    # convert data to dimension for input in model
        data=data.float()                 # convert tensor to float tensor
        pred = model(data)                # get model prediction

    fig,ax = plt.subplots(5,3, figsize=(30,20))   # create plot
    for j in range(0,5):                          # plot 15 samples  
      for k in range(0,3):
        ax[j,k].plot(np.arange(10),data_copy[3*j+k], label='Sine')
        ax[j,k].plot(np.arange(10),pred[3*j+k], label="Cosine label") 
        ax[j,k].plot(np.arange(10),labels[3*j+k], label="Predicted cosine")
        ax[j,k].set_title("RNN prediction case {}".format(k+3*j))
        ax[j,k].legend()

"""###GRU"""

n_iters = 3000      # number of iterations
batch_size = 500    # batch size

train = data_utils.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())   # create train dataset from train data and labels
train_loader = torch.utils.data.DataLoader(dataset=train,batch_size=batch_size,shuffle=True)             # create dataloader, shuffle data 

test = data_utils.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())      # create test dataset from test data and labels
test_loader = torch.utils.data.DataLoader(dataset=test,batch_size=len(X_test), shuffle=False)            # we dont want batches in test data, just a dataloader, so we set batch size equal to our test samples

num_epochs = int(n_iters / (len(X_train) / batch_size))                                                  # compute number of epochs from number of iterations we want, batch size and number of train samples  
print("Epochs for training:",num_epochs)
print("Batch size:",batch_size)

class GRUModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):
        super(GRUModel, self).__init__()
        self.hidden_dim = hidden_dim                                                                    # dimension of hidden layer
        self.layer_dim = layer_dim                                                                      # number of reccurent layers
        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)                                                     # linear layer

    def forward(self, x):

        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()                   # hidden state initialized with zeros 
        out, hn = self.gru(x, h0.detach())                                                              # detach the hidden state to prevent exploding/vanishing gradients 
        out = self.fc(out[:, -1, :])                                                                    # keep only last time step of hidden state
        return out

input_dim = 1        # dimension of input
hidden_dim = 100     # hidden layer dimension 
layer_dim = 1        # number of hidden layers 
output_dim = 10      # dimemnsion of output 

model = GRUModel(input_dim, hidden_dim, layer_dim, output_dim)    # initialize output
criterion = nn.MSELoss()                                          # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)         # optimizer

# train model:
for j in range(num_epochs):
  for i, (data, labels) in enumerate(train_loader):
        optimizer.zero_grad()                            # set gradients to zero
        data=data.view(len(data),10,1)                   # addapt dimension to input to model   
        data=data.float()                                # convert tensor to to float tensor
        outputs = model(data)                            # give input to model and get output                    
        loss = criterion(outputs, labels)                # calculate loss 

        if j%100 == 0:                                   # print train loss every 100 epochs                           
          print('Train loss', loss.item())

        loss.backward()                                  # compute gradient   
        optimizer.step()                                 # update parameters

        if j%100 == 0:                                   # print test loss every 100 epochs
          for i, (data, labels) in enumerate(test_loader):
            with torch.no_grad():                        # disable gradient calculation as we will not call torch.backward()
              data=data.view(len(data),10,1)
              data=data.float()
              pred = model(data)
              loss = criterion(pred,labels)
              
              print('Test loss:', loss.item())
              print()

# plot sum of the sine sequences and the predicted cosine
for i, (data, labels) in enumerate(test_loader):   
    data_copy=data
    with torch.no_grad():                 # do not compute gradients
        data=data.view(len(data),10,1)    # convert data to dimension for input in model
        data=data.float()                 # convert tensor to float tensor
        pred = model(data)                # get model prediction

    fig,ax = plt.subplots(5,3, figsize=(30,20))   # create plot
    for j in range(0,5):                          # plot 15 samples  
      for k in range(0,3):
        ax[j,k].plot(np.arange(10),data_copy[3*j+k], label='Sine')
        ax[j,k].plot(np.arange(10),pred[3*j+k], label="Cosine label") 
        ax[j,k].plot(np.arange(10),labels[3*j+k], label="Predicted cosine")
        ax[j,k].set_title("GRU prediction case {}".format(k+3*j))
        ax[j,k].legend()

"""###LSTM"""

n_iters = 3000      # number of iterations
batch_size = 500    # batch size

train = data_utils.TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float())   # create train dataset from train data and labels
train_loader = torch.utils.data.DataLoader(dataset=train,batch_size=batch_size,shuffle=True)             # create dataloader, shuffle data 

test = data_utils.TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float())      # create test dataset from test data and labels
test_loader = torch.utils.data.DataLoader(dataset=test,batch_size=len(X_test), shuffle=False)            # we dont want batches in test data, just a dataloader, so we set batch size equal to our test samples

num_epochs = int(n_iters / (len(X_train) / batch_size))                                                  # compute number of epochs from number of iterations we want, batch size and number of train samples  
print("Epochs for training:",num_epochs)
print("Batch size:",batch_size)

class LSTMModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim                                                                    # dimension of hidden layer
        self.layer_dim = layer_dim                                                                      # number of reccurent layers
        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)                                                     # linear layer

    def forward(self, x):

        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()                   # hidden state initialized with zeros 
        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()
        out, hn = self.lstm(x, (h0.detach(), c0.detach()))                                                # detach the hidden state to prevent exploding/vanishing gradients 
        out = self.fc(out[:, -1, :])                                                                    # keep only last time step of hidden state
        return out

input_dim = 1        # dimension of input
hidden_dim = 100     # hidden layer dimension 
layer_dim = 1        # number of hidden layers 
output_dim = 10      # dimemnsion of output 

model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)    # initialize output
criterion = nn.MSELoss()                                          # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)         # optimizer

# train model:
for j in range(num_epochs):
  for i, (data, labels) in enumerate(train_loader):
        optimizer.zero_grad()                            # set gradients to zero
        data=data.view(len(data),10,1)                   # addapt dimension to input to model   
        data=data.float()                                # convert tensor to to float tensor
        outputs = model(data)                            # give input to model and get output                    
        loss = criterion(outputs, labels)                # calculate loss 

        if j%100 == 0:                                   # print train loss every 100 epochs                           
          print('Train loss', loss.item())

        loss.backward()                                  # compute gradient   
        optimizer.step()                                 # update parameters

        if j%100 == 0:                                   # print test loss every 100 epochs
          for i, (data, labels) in enumerate(test_loader):
            with torch.no_grad():                        # disable gradient calculation as we will not call torch.backward()
              data=data.view(len(data),10,1)
              data=data.float()
              pred = model(data)
              loss = criterion(pred,labels)
              
              print('Test loss:', loss.item())
              print()

# plot sum of the sine sequences and the predicted cosine
for i, (data, labels) in enumerate(test_loader):   
    data_copy=data
    with torch.no_grad():                 # do not compute gradients
        data=data.view(len(data),10,1)    # convert data to dimension for input in model
        data=data.float()                 # convert tensor to float tensor
        pred = model(data)                # get model prediction

    fig,ax = plt.subplots(5,3, figsize=(30,20))   # create plot
    for j in range(0,5):                          # plot 15 samples  
      for k in range(0,3):
        ax[j,k].plot(np.arange(10),data_copy[3*j+k], label='Sine')
        ax[j,k].plot(np.arange(10),pred[3*j+k], label="Cosine label") 
        ax[j,k].plot(np.arange(10),labels[3*j+k], label="Predicted cosine")
        ax[j,k].set_title("LSTM prediction case {}".format(k+3*j))
        ax[j,k].legend()

"""##Βήμα 9: Load data and split in train, validation and test set

"""

!git clone https://github.com/Jakobovski/free-spoken-digit-dataset.git

def class_distribution(y, title):   # function to plot number of samples per digit and print number of samples per digit
  fig = plt.figure()
  plt.title(title)
  y1=[]
  y = np.array(y)
  for i in range(0,10):
    y1.append((np.where(y == i)[0]).shape[0])       # append number of labels for digit i
  plt.ylabel('Occurance')
  plt.xlabel('Digits')
  plt.bar(np.arange(0,10),y1, align='center', width=0.6)
  for i in range(len(y1)):                          # print number of samples per digit
    print('Number of samples for digit {}:'.format(i),y1[i])
  print('Total number of samples:', len(y))
  plt.show()

import os
from glob import glob

import librosa
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm


def parse_free_digits(directory):
    # Parse relevant dataset info
    files = glob(os.path.join(directory, "*.wav"))
    #print(files)
    fnames = [f.split("/")[-1].split(".")[0].split("_") for f in files]
    #fnames = [f.split("\\")[1].split(".")[0].split("_") for f in files]
    #print(fnames)
    ids = [f[2] for f in fnames]
    y = [int(f[0]) for f in fnames]
    speakers = [f[1] for f in fnames]
    _, Fs = librosa.core.load(files[0], sr=None)

    def read_wav(f):
        wav, _ = librosa.core.load(f, sr=None)

        return wav

    # Read all wavs
    wavs = [read_wav(f) for f in files]

    # Print dataset info
    print("Total wavs: {}. Fs = {} Hz".format(len(wavs), Fs))

    return wavs, Fs, ids, y, speakers


def extract_features(wavs, n_mfcc=6, Fs=8000):
    # Extract MFCCs for all wavs
    window = 30 * Fs // 1000
    step = window // 2
    frames = [
        librosa.feature.mfcc(
            wav, Fs, n_fft=window, hop_length=window - step, n_mfcc=n_mfcc
        ).T

        for wav in tqdm(wavs, desc="Extracting mfcc features...")
    ]

    print("Feature extraction completed with {} mfccs per frame".format(n_mfcc))

    return frames


def split_free_digits(frames, ids, speakers, labels):
    print("Splitting in train test split using the default dataset split")
    # Split to train-test
    X_train, y_train, spk_train = [], [], []
    X_test, y_test, spk_test = [], [], []
    test_indices = ["0", "1", "2", "3", "4"]

    for idx, frame, label, spk in zip(ids, frames, labels, speakers):
        if str(idx) in test_indices:
            X_test.append(frame)
            y_test.append(label)
            spk_test.append(spk)
        else:
            X_train.append(frame)
            y_train.append(label)
            spk_train.append(spk)

    return X_train, X_test, y_train, y_test, spk_train, spk_test


def make_scale_fn(X_train):
    # Standardize on train data
    scaler = StandardScaler()
    scaler.fit(np.concatenate(X_train))
    print("Normalization will be performed using mean: {}".format(scaler.mean_))
    print("Normalization will be performed using std: {}".format(scaler.scale_))
    def scale(X):
        scaled = []

        for frames in X:
            scaled.append(scaler.transform(frames))
        return scaled
    return scale


def parser(directory, n_mfcc=6):
    wavs, Fs, ids, y, speakers = parse_free_digits(directory)
    frames = extract_features(wavs, n_mfcc=n_mfcc, Fs=Fs)
    X_train, X_test, y_train, y_test, spk_train, spk_test = split_free_digits(
        frames, ids, speakers, y
    )

    return X_train, X_test, y_train, y_test, spk_train, spk_test

"""###Load train and test data 


"""

X_tr, X_test, y_tr, y_test, spk_train, spk_test =parser('free-spoken-digit-dataset/recordings')
#X_tr, X_test, y_tr, y_test, spk_train, spk_test =parser('/content/drive/MyDrive/Colab Notebooks/pattern_rec/2ο Εργαστήριο/recordings')

class_distribution(y_tr,'Train Samples Initial')   # print number of digits per sample for train set 
class_distribution(y_test,'Test Samples Initial')  # print number of digits per sample for test set

"""###Split in train and validation set and normalize data:"""

X_train, X_dev, y_train, y_dev = train_test_split(X_tr, y_tr, test_size = 0.2, random_state=42, stratify = y_tr)   # split train set into train and validation set
print("If using all data to calculate normalization statistics")
scale_fn = make_scale_fn(X_train + X_dev + X_test)
print("If using X_train + X_dev to calculate normalization statistics")
scale_fn = make_scale_fn(X_train + X_dev)
print("If using X_train to calculate normalization statistics")
scale_fn = make_scale_fn(X_train)                                 # calculate scaler with standardization
X_train = scale_fn(X_train)                                       # normalize train data
X_dev = scale_fn(X_dev)                                           # normalize validation data
X_test = scale_fn(X_test)                                         # mormalize test data

class_distribution(y_train,'Train Samples')      # print and show in plot number of samples per digit in train set
class_distribution(y_dev,'Validation Samples')   # print and show in plot number of samples per digit in validation set
class_distribution(y_test,'Test Samples')        # print and show in plot number of samples per digit in test set

"""## Βήμα 10: Initialize GMM-HMMs

###Create a GMM-HMM model class:
"""

from pomegranate import *

class GMM_HMM:                                                        # class for HMM model for one digit
    def __init__(self,X,n_states,n_mixtures,gmm=True):
      self.X=X                                                        # data from a single digit (can be a numpy array)
      self.n_states=n_states                                          # the number of HMM states
      self.n_mixtures=n_mixtures                                      # the number of Gaussians
      self.gmm= gmm                                                   # whether to use GMM or plain Gaussian

      if(n_mixtures==1):                                              # if eant only one GMM per state we don't have a GMM
        self.gmm = False

      self.dists = []                                                 # list of probability distributions for the HMM states
      for i in range(self.n_states):
          if self.gmm:
              a = GeneralMixtureModel.from_samples(MultivariateGaussianDistribution, self.n_mixtures, np.float_(self.X))
          else:
              a = MultivariateGaussianDistribution.from_samples(X)
          self.dists.append(a)

      self.trans_mat = np.zeros((self.n_states,self.n_states))
      
      for i in range(self.n_states):
          for j in range(self.n_states):
              if j >= i and j <= i+1:
                  if j == i and j == self.n_states - 1: 
                    self.trans_mat[i][j] = 1
                  else:
                    self.trans_mat[i][j] = 0.5

      self.starts = np.zeros(self.n_states)                       # starting probability matrix
      self.ends = np.zeros(self.n_states)                         # ending probability matrix
      self.starts[0]=1                                            # probability of initial state 0 is 1                                  
      self.ends[-1]=1                                             # probability of ending in final state is 1
      
      self.model = HiddenMarkovModel.from_matrix(self.trans_mat, self.dists, self.starts, self.ends, state_names=['s{}'.format(i) for i in range(n_states)])  # define the GMM-HMM
      
    
    def fit(self,data,max_iterations=5):                          # function to fit the model
      self.model.fit(data, max_iterations=max_iterations)
      return self

    def predict(self,sample):
      logp, _ = self.model.viterbi(sample)                        # run viterbi algorithm and return log-probability
      return logp

def get_digit_indexes(y,digit):
    '''Takes a table of labels returns all indexes of the table where the label corresponds to a specific digit/class.

    Args:
        digit (int): The digit whose indexes we are searching.
        y (np.ndarray): Labels for dataset (nsamples)

    Returns:
        indexes (np.ndarray): A 1d table with the indexes found.
    '''
    indexes=[]
    for index,label in enumerate(y):        #loop through labels
        val=int(label)
        if val==digit:                      # if the label corresponds to the digit
            indexes.append(index)           # add the index to the table
    return indexes

"""###Initialize one GMM-HMM per digit"""

def create_hmms(X_train, y_train, n_states=4,n_mixtures=4):                             # function to initialize one hmm for each digit, 
  #parameters: X_train: train data, y_train: train labels, n_states (number of HMM states) and n_mixtures (number of GMM mixtures per HMM)
  # returns a list with 10 HMMs
 
  hmms=[]                                                                      # list to hold 10 HMMs, one for each digit

  X_digits=[]                                                                  # list to hold data for each digit seperately
  for i in range(10):
    X_digits.append(np.take(X_train, get_digit_indexes(y_train,i), axis=0))    # X_digits[i] keeps data for digit i

  # initialize 10 HMMs:
  for i in range(10):
    X=np.vstack(X_digits[i])                                                   # get data for digit i and stack all samples of digit on one array 
    hmm=GMM_HMM(X,n_states,n_mixtures)                                         # initialize HMMs
    hmms.append(hmm)                                                           # append hmm to list of HMMs       

  return hmms

"""##Βήμα 11: Train GMM-HMMs"""

def fit_hmms(X_train, y_train,hmms,max_iterations=5): # function to train hmm models
  # parameters: X_train: train data, y_train: train labels, hmms: list of hmms to train, max_iterations: maximum number of iterations for training
  
  X_digits=[]                                                                  # list to hold data for each digit seperately
  for i in range(10):
    X_digits.append(np.take(X_train, get_digit_indexes(y_train,i), axis=0))    # X_digits[i] keeps data for digit i
  
  for i in range(0,len(hmms)):
    hmms[i].fit(X_digits[i],max_iterations)

"""##Βήμα 12: Calculate Best Model and evaluate on test set"""

def evaluate_hmms(hmms, X_validation, y_validation):
    predictions =[]                                               # list to keep predictions
    for i in range(len(y_validation)):                            # for all validation samples
        logps = []                                                # list to keep log likelihood calculated by each model
        for j in range(10):                                     
            logp = hmms[j].predict(X_validation[i])               # run viterbi algorithm and return log-probability
            logps.append(logp)                                    # append log-probability to list                                   
        logps=np.array(logps)                   
        predictions.append(np.argmax(logps))                      # prediction is the digit whose HMM maximizes the log-probability
    predictions=np.array(predictions)
    val=np.array(y_validation)
    accuracy = np.sum(predictions == val)/len(y_validation) # calculate model accuracy based on the number of correct predictions
    return accuracy

"""###Ηyperparameter tuning:

Δοκιμάζουμε την δημιουργία μοντέλων με 1 ως 4 καταστάσεις και 1 ως 5 γκαουσιανές κατανομές, καθώς και με διαφορετικούς αριθμούς μέγιστων iterations.
"""

def grid_search(X_train,y_train,X_validation,y_validation,states,mixtures,max_iterations):  # function to train HMMs with different parameters and return best combination
  # parameters: X_train,  y_train: train data and labels, X_validation, y_validation: validation data and labels, states: list of numbers of HMM states, mixtures: list of numbers of GMMs for HMMs, max_iterations: maximum number of iterations for model fitting
  best_accuracy=0.0
  best_result=None
  for num_states in states:                                                     # repeat for each combination of parameters:
    for num_mixtures in mixtures:
      for max_it in max_iterations:
        hmms=create_hmms(X_train, y_train,num_states,num_mixtures)              # create HMMs
        fit_hmms(X_train,y_train,hmms,max_it)                                   # fit HMMs
        accuracy = evaluate_hmms(hmms, X_validation, y_validation)              # calculate accuracy of prediction
        print("HMM with: number of HMM states: "+str(num_states)+", number of GMMs: "+str(num_mixtures)+", maximum number of iterations: "+str(max_it)+" has accuracy: "+str(accuracy))   # save results of evalutation to print in the end

        if accuracy>best_accuracy:                                              # keep best accuracy so far and the parameters that achieved it:
          best_accuracy=accuracy
          best_result="Best parameters: number of HMM states: "+str(num_states)+", number of GMMs: "+str(num_mixtures)+", maximum number of iterations: "+str(max_it)
  
  return best_result

grid_search(X_train,y_train,X_dev,y_dev,[1,2,3,4],[1,2,3,4,5],[5,10,50,100])

"""###Create the HMM model for the best parametes and test accuracy in test set"""

hmms=create_hmms(X_train, y_train,4,5)              # create HMMs
fit_hmms(X_train,y_train,hmms,5)                   # fit HMMs

accuracy = evaluate_hmms(hmms, X_test, y_test)              # calculate accuracy of prediction
print("Model has accuracy in test set: ", accuracy)

"""##Βήμα 13: Confusion matrices"""

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title, fontsize=17)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

def calculate_confusion_matrix(hmms,X,y):
    cm=np.zeros((10,10))                               # confusion matrix, rows are the classes, columns are the predicted label
    for i in range(len(y)):                            # for all samples
        logps = []                                     # list to keep log likelihood calculated by each model
        for j in range(10):                                     
            logp = hmms[j].predict(X[i])               # run viterbi algorithm and return log-probability
            logps.append(logp)                         # append log-probability to list                                   
        logps=np.array(logps)                   
        prediction=np.argmax(logps)                    # prediction is the digit whose HMM maximizes the log-probability
        cm[y[i]][prediction]+=1                        # increase the value of the right cell of confusion matrix 
    return cm

cm_dev=calculate_confusion_matrix(hmms,X_dev,y_dev)     # calculate confusion matrix for validation set
cm_test=calculate_confusion_matrix(hmms,X_test,y_test)  # calculate confusion matrix for test set

plt.rcParams['figure.figsize'] = [8, 8]        # ensures confusion matrix plot will not be too small

plot_confusion_matrix(cm_dev,set(y_dev),normalize=True,title="Confusion matrix for validation set")  # prin and plot confusion matrix for validation set

plot_confusion_matrix(cm_test,set(y_test),normalize=True,title="Confusion matrix for test set")  # prin and plot confusion matrix for test set

"""##Βήμα 14: LSTM implementation

###Initialize FrameLevelDataset and create dataloaders
"""

import os
import numpy as np
import torch
from torch.utils.data import Dataset
import torch.nn as nn


class FrameLevelDataset(Dataset):
    def __init__(self, feats, labels):
        """
            feats: Python list of numpy arrays that contain the sequence features.
                   Each element of this list is a numpy array of shape seq_length x feature_dimension
            labels: Python list that contains the label for each sequence (each label must be an integer)
        """
        self.lengths = np.array([len(item) for item in feats]) # Find the lengths 
        self.feats = self.zero_pad_and_stack(feats)
        if isinstance(labels, (list, tuple)):
            self.labels = np.array(labels).astype('int64')

    def zero_pad_and_stack(self, x):
        """
            This function performs zero padding on a list of features and forms them into a numpy 3D array
            returns
                padded: a 3D numpy array of shape num_sequences x max_sequence_length x feature_dimension
        """
        padded = []
        # --------------- Insert your code here ---------------- #
        max_sequence_length=max([len(item) for item in x])
        padded=[np.pad(item, ((0,max_sequence_length-item.shape[0]),(0,0)), mode='constant') for item in x]  # add zeros only
        padded=np.array(padded)
        return padded

    def __getitem__(self, item):
        return self.feats[item], self.labels[item], self.lengths[item]

    def __len__(self):
        return len(self.feats)

train = FrameLevelDataset(X_train, y_train)     # create trainig dataset
validation = FrameLevelDataset(X_dev, y_dev)    # create validation dataset
test = FrameLevelDataset(X_test, y_test)     # create test dataset

batch_size=16
train_loader = torch.utils.data.DataLoader(dataset=train,batch_size=batch_size,shuffle=True)            # create dataloader, shuffle data 
validation_loader = torch.utils.data.DataLoader(dataset=validation,batch_size=batch_size,shuffle=True)  # create dataloader, shuffle data 
test_loader = torch.utils.data.DataLoader(dataset=test,batch_size=batch_size,shuffle=True)             # create dataloader, shuffle data

"""###1: Create simple LSTM class"""

class BasicLSTM(nn.Module):
    def __init__(self, input_dim, rnn_size, output_dim, num_layers, bidirectional=False, dropout=0):
        super(BasicLSTM, self).__init__()
        self.bidirectional = bidirectional
        self.feature_size = rnn_size * 2 if self.bidirectional else rnn_size
        self.num_layers = num_layers
        self.hidden_size=rnn_size
        self.dropout=dropout

        # --------------- Insert your code here ---------------- #
        # Initialize the LSTM, Dropout, Output layers

        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=rnn_size, num_layers=num_layers, batch_first=True, bidirectional=self.bidirectional, dropout=self.dropout) #lstm
        self.linear = nn.Linear(self.feature_size, output_dim)

    def forward(self, x, lengths):
        """ 
            x : 3D numpy array of dimension N x L x D
                N: batch index
                L: sequence index
                D: feature index

            lengths: N x 1
         """
        
        # --------------- Insert your code here ---------------- #
        
        # You must have all of the outputs of the LSTM, but you need only the last one (that does not exceed the sequence length)
        # To get it use the last_timestep method
        # Then pass it through the remaining network
        
        if not self.bidirectional:
          factor=1
        else:
          factor=2
          
        h_0 = torch.zeros(self.num_layers*factor, x.size(0), self.hidden_size) #hidden state
        c_0 = torch.zeros(self.num_layers*factor, x.size(0), self.hidden_size) #internal state
        
        output, (hn, cn) = self.lstm(x, (h_0, c_0))  #lstm with input, hidden, and internal state
        last_timestep_output=self.last_timestep(output, lengths, self.bidirectional)
        last_outputs = self.linear(last_timestep_output)
        
        return last_outputs

    def last_timestep(self, outputs, lengths, bidirectional=False):
        """
            Returns the last output of the LSTM taking into account the zero padding
        """
        if bidirectional:
            forward, backward = self.split_directions(outputs)
            last_forward = self.last_by_index(forward, lengths)
            last_backward = backward[:, 0, :]
            # Concatenate and return - maybe add more functionalities like average
            return torch.cat((last_forward, last_backward), dim=-1)

        else:
            return self.last_by_index(outputs, lengths)

    @staticmethod
    def split_directions(outputs):
        direction_size = int(outputs.size(-1) / 2)
        forward = outputs[:, :, :direction_size]
        backward = outputs[:, :, direction_size:]
        return forward, backward

    @staticmethod
    def last_by_index(outputs, lengths):
        # Index of the last output for each sequence.
        idx = (lengths - 1).view(-1, 1).expand(outputs.size(0),
                                               outputs.size(2)).unsqueeze(1)
        return outputs.gather(1, idx).squeeze()

"""###2: Initialize simple LSTM"""

input_dim = 6        # dimension of input: number of MFCCs
rnn_dim = 128        # hidden layer dimension  
output_dim = 10      # dimemnsion of output 
num_layers = 2       # number of hidden layers
learning_rate=0.001  # learning rate of optimizer 
num_epochs=30        # number of training epochs

model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers)             # initialize model
criterion = nn.CrossEntropyLoss()                                         # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)        # optimizer

"""###3: Train LSTM, print only training loss per epoch"""

def train_model(model,optimizer,criterion,num_epochs,train_loader,validation_loader=None,validate=True):
  
  # _____train model:_____
 
  train_losses=[]                                          # array to hold mean train losses per epoch
  validation_losses=[]
    
  for j in range(num_epochs):
    train_loss_batch=[]                                    # array to hold train losses per batch
    
    for i, (X,y,length) in enumerate(train_loader):

          optimizer.zero_grad()                            # set gradients to zero

          outputs = model(X,length)                        # give input to model and get output                    
          
          loss = criterion(outputs, y)                     # calculate loss 

          loss.backward()                                  # compute gradient   
          optimizer.step()                                 # update parameters

          train_loss_batch.append(loss.item())             # add batch loss to array 
    
    mean_training_loss=np.mean(train_loss_batch)           # calculate average training loss for epoch 
    train_losses.append(mean_training_loss)
    
    # _____evaluate on validation set:_____
    
    if validate: 
      validation_loss_batch=[]                              # array to hold validation losses per batch
      predictions=np.array([])                                 # array to hold predicted labels for validation set
      labels=np.array([])                                      # array to hold actual labels for validation set
      
      for i, (X,y,length) in enumerate(validation_loader):
        with torch.no_grad():                               # disable gradient calculation as we will not call torch.backward()
          pred = model(X,length)                            # get predictions
          loss = criterion(pred,y)                          # calculate loss 
          validation_loss_batch.append(loss.item())         # add batch loss to array
          predict=torch.argmax(pred,dim=1).numpy()          # get predicted label
          predictions=np.concatenate((predictions,predict)) # add to predictions
          labels=np.concatenate((labels,y))

      mean_validation_loss=np.mean(validation_loss_batch)   # calculate mean loss in epoch
      validation_losses.append(mean_validation_loss)
  
    print("Epoch {}: Mean training loss per epoch: {}".format(j,mean_training_loss))
    if validate:
      print("Epoch {}: Mean validation loss per epoch: {}".format(j,mean_validation_loss))
    print('--------------------------------------------------------------------------')
  if validate:
    accuracy=np.sum(predictions == labels)/len(labels)
    return accuracy, train_losses, validation_losses
  else:
    return train_losses

train_losses=train_model(model,optimizer,criterion,num_epochs,train_loader,validate=False)

x=np.arange(num_epochs)
plt.plot(x,train_losses)
plt.title('Training loss per epoch',fontsize=15)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.show()

"""###4: Train lstm, print training loss and validation loss per epoch"""

model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers)             # re-initialize model
criterion = nn.CrossEntropyLoss()                                         # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)        # optimizer

accuracy, train_losses, validation_losses =train_model(model,optimizer,criterion,num_epochs,train_loader,validation_loader,validate=True)
print('Model has accuracy on validation data: ',accuracy)

x=np.arange(num_epochs)
plt.plot(x,train_losses,label='Training loss')
plt.plot(x,validation_losses,label='Validation loss')
plt.title('Training and validation loss per epoch',fontsize=15)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""###5: Addition of Dropout and L2 Regulization

Δοκιμάζουμε διαφορετικές τιμές για dropout και L2 regularization:

####Dropout 0.2, weight decay 0.00001
"""

dropout=0.2
weight_decay=0.00001
model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers, dropout=dropout)                       # initialize model
criterion = nn.CrossEntropyLoss()                                                                    # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)        # optimizer with L2 normalizaton

accuracy, train_losses, validation_losses =train_model(model,optimizer,criterion,num_epochs,train_loader,validation_loader,validate=True)
print('Model has accuracy on validation data: ',accuracy)

x=np.arange(num_epochs)
plt.plot(x,train_losses,label='Training loss')
plt.plot(x,validation_losses,label='Validation loss')
plt.title('Training and validation loss per epoch, with Dropout={} and L2 Regularization with weight decay={}'.format(dropout,weight_decay),fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""####Dropout 0.3, weight decay 0.00001"""

dropout=0.3
weight_decay=0.00001
model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers, dropout=dropout)                       # initialize model
criterion = nn.CrossEntropyLoss()                                                                    # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)        # optimizer with L2 normalizaton

accuracy, train_losses, validation_losses =train_model(model,optimizer,criterion,num_epochs,train_loader,validation_loader,validate=True)
print('Model has accuracy on validation data: ',accuracy)

x=np.arange(num_epochs)
plt.plot(x,train_losses,label='Training loss')
plt.plot(x,validation_losses,label='Validation loss')
plt.title('Training and validation loss per epoch, with Dropout={} and L2 Regularization with weight decay={}'.format(dropout,weight_decay),fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""####Dropout 0.4, weight decay 0.00001"""

dropout=0.4
weight_decay=0.00001
model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers, dropout=dropout)                       # initialize model
criterion = nn.CrossEntropyLoss()                                                                    # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)        # optimizer with L2 normalizaton

accuracy, train_losses, validation_losses =train_model(model,optimizer,criterion,num_epochs,train_loader,validation_loader,validate=True)
print('Model has accuracy on validation data: ',accuracy)

x=np.arange(num_epochs)
plt.plot(x,train_losses,label='Training loss')
plt.plot(x,validation_losses,label='Validation loss')
plt.title('Training and validation loss per epoch, with Dropout={} and L2 Regularization with weight decay={}'.format(dropout,weight_decay),fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""####Dropout 0.6, weight decay 0.00001"""

dropout=0.6
weight_decay=0.00001
model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers, dropout=dropout)                       # initialize model
criterion = nn.CrossEntropyLoss()                                                                    # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)        # optimizer with L2 normalizaton

accuracy, train_losses, validation_losses =train_model(model,optimizer,criterion,num_epochs,train_loader,validation_loader,validate=True)
print('Model has accuracy on validation data: ',accuracy)

x=np.arange(num_epochs)
plt.plot(x,train_losses,label='Training loss')
plt.plot(x,validation_losses,label='Validation loss')
plt.title('Training and validation loss per epoch, with Dropout={} and L2 Regularization with weight decay={}'.format(dropout,weight_decay),fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""####Dropout 0.2 weight decay 0.001"""

dropout=0.2
weight_decay=0.001
model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers, dropout=dropout)                      # initialize model
criterion = nn.CrossEntropyLoss()                                                                   # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)       # optimizer with L2 normalizaton

accuracy, train_losses, validation_losses =train_model(model,optimizer,criterion,num_epochs,train_loader,validation_loader,validate=True)
print('Model has accuracy on validation data: ',accuracy)

x=np.arange(num_epochs)
plt.plot(x,train_losses,label='Training loss')
plt.plot(x,validation_losses,label='Validation loss')
plt.title('Training and validation loss per epoch, with Dropout={} and L2 Regularization with weight decay={}'.format(dropout,weight_decay),fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""####Dropout 0.4 weight decay 0.001"""

dropout=0.4
weight_decay=0.001
model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers, dropout=dropout)                      # initialize model
criterion = nn.CrossEntropyLoss()                                                                   # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)       # optimizer with L2 normalizaton

accuracy, train_losses, validation_losses =train_model(model,optimizer,criterion,num_epochs,train_loader,validation_loader,validate=True)
print('Model has accuracy on validation data: ',accuracy)

x=np.arange(num_epochs)
plt.plot(x,train_losses,label='Training loss')
plt.plot(x,validation_losses,label='Validation loss')
plt.title('Training and validation loss per epoch, with Dropout={} and L2 Regularization with weight decay={}'.format(dropout,weight_decay),fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""####Dropout 0.6 weight decay 0.001"""

dropout=0.6
weight_decay=0.001
model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers, dropout=dropout)                      # initialize model
criterion = nn.CrossEntropyLoss()                                                                   # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)       # optimizer with L2 normalizaton

accuracy, train_losses, validation_losses =train_model(model,optimizer,criterion,num_epochs,train_loader,validation_loader,validate=True)
print('Model has accuracy on validation data: ',accuracy)

x=np.arange(num_epochs)
plt.plot(x,train_losses,label='Training loss')
plt.plot(x,validation_losses,label='Validation loss')
plt.title('Training and validation loss per epoch, with Dropout={} and L2 Regularization with weight decay={}'.format(dropout,weight_decay),fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""###6: Early Stopping and Checkpoints"""

dropout=0.4
weight_decay=0.00001
num_epochs=50
model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers, dropout=dropout)                       # initialize model
criterion = nn.CrossEntropyLoss()                                                                    # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)        # optimizer with L2 normalizaton

def train_model_early_stopping(model,optimizer,criterion,num_epochs,train_loader,validation_loader=None,validate=True,patience=4):

  # ______train model:______
  
  train_losses=[]                                          # array to hold mean train losses per epoch
  validation_losses=[]

  last_loss = 1000                                         # variable to hold loss of previous epoch, initialize to abig number
  trigger_times = 0                                        # number of times loss has not decreased
  epochs_performed=0

  for j in range(num_epochs):
    epochs_performed+=1                                    # increase number of epochs for which we have trained the model 
    train_loss_batch=[]                                    # array to hold train losses per batch
    
    for i, (X,y,length) in enumerate(train_loader):

          optimizer.zero_grad()                            # set gradients to zero

          outputs = model(X,length)                        # give input to model and get output                    
          
          loss = criterion(outputs, y)                     # calculate loss 

          loss.backward()                                  # compute gradient   
          optimizer.step()                                 # update parameters

          train_loss_batch.append(loss.item())             # add batch loss to array 
    
    mean_training_loss=np.mean(train_loss_batch)           # calculate average training loss for epoch 
    train_losses.append(mean_training_loss)
    
    # _____evaluate on validation set:_____
   
    if validate:    
      validation_loss_batch=[]                              # array to hold validation losses per batch
      predictions=np.array([])                              # array to hold predicted labels for validation set
      labels=np.array([])                                     # array to hold actual labels for validation set
      
      for i, (X,y,length) in enumerate(validation_loader):
        with torch.no_grad():                               # disable gradient calculation as we will not call torch.backward()
          pred = model(X,length)                            # get predictions
          loss = criterion(pred,y)                          # calculate loss 
          validation_loss_batch.append(loss.item())         # add batch loss to array
          predict=torch.argmax(pred,dim=1).numpy()          # get predicted label
          predictions=np.concatenate((predictions,predict)) # add to predictions
          labels=np.concatenate((labels,y))

      mean_validation_loss=np.mean(validation_loss_batch)   # calculate mean loss in epoch
      validation_losses.append(mean_validation_loss)
  
    print("Epoch {}: Mean training loss per epoch: {}".format(j,mean_training_loss))
    if validate:
      print("Epoch {}: Mean validation loss per epoch: {}".format(j,mean_validation_loss))

    # _____early stopping:_____  
    
    if mean_validation_loss >= last_loss:
            
      trigger_times += 1
      print('Number of times validation loss has not decreased:', trigger_times)

      if trigger_times >= patience:
          print('Early stopping...')
          if validate:
            accuracy=np.sum(predictions == labels)/len(labels)
            return accuracy, train_losses, validation_losses , epochs_performed
          else:
            return train_losses

    else:
      trigger_times = 0           
      torch.save(model.state_dict(), './model.pt')   # create checkpoint

    last_loss = mean_validation_loss

    print('--------------------------------------------------------------------------')

  if validate:
    accuracy=np.sum(predictions == labels)/len(labels)
    return accuracy, train_losses, validation_losses, epochs_performed
  else:
    return train_losses, epochs_performed

accuracy, train_losses, validation_losses, epochs_performed =train_model_early_stopping(model,optimizer,criterion,num_epochs,train_loader,validation_loader,validate=True,patience=3)
print('Model has accuracy on validation data: ',accuracy)

x=np.arange(epochs_performed)
plt.plot(x,train_losses,label='Training loss')
plt.plot(x,validation_losses,label='Validation loss')
plt.title('Training and validation loss per epoch, with Dropout={} and L2 Regularization with weight decay={}'.format(dropout,weight_decay),fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""###7: Bidirectional LSTM"""

dropout=0.4
weight_decay=0.00001
num_epochs=50
model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers, dropout=dropout, bidirectional=True)   # initialize model
criterion = nn.CrossEntropyLoss()                                                                    # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)        # optimizer with L2 normalizaton

accuracy, train_losses, validation_losses, epochs_performed =train_model_early_stopping(model,optimizer,criterion,num_epochs,train_loader,validation_loader,validate=True,patience=3)
print('Model has accuracy on validation data: ',accuracy)

x=np.arange(epochs_performed)
plt.plot(x,train_losses,label='Training loss')
plt.plot(x,validation_losses,label='Validation loss')
plt.title('Bidirectional LSTM training and validation loss per epoch, with Dropout={} and L2 Regularization with weight decay={}'.format(dropout,weight_decay),fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""###Best model

####Train model:
"""

dropout=0.4
weight_decay=0.00001
num_epochs=50
best_model = BasicLSTM(input_dim, rnn_dim, output_dim, num_layers, dropout=dropout, bidirectional=False)   # initialize model
criterion = nn.CrossEntropyLoss()                                                                    # loss function
optimizer = torch.optim.Adam(best_model.parameters(), lr=learning_rate, weight_decay=weight_decay)        # optimizer with L2 normalizaton

accuracy, train_losses, validation_losses, epochs_performed =train_model_early_stopping(best_model,optimizer,criterion,num_epochs,train_loader,validation_loader,validate=True,patience=3)
print('Model has accuracy on validation data: ',accuracy)

"""####Plot training and test loss"""

# plot training and validation loss
x=np.arange(epochs_performed)
plt.plot(x,train_losses,label='Training loss')
plt.plot(x,validation_losses,label='Validation loss')
plt.title('LSTM training and validation loss per epoch, with Dropout={} and L2 Regularization with weight decay={}'.format(dropout,weight_decay),fontsize=12)
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""####Calculate accuracy on validation and test set"""

# get accuracy and predictions for validation set:

validation_predictions=np.array([])                         # array to hold predicted labels for validation set
validation_labels=np.array([])                              # array to hold actual labels for validation set
for i, (X,y,length) in enumerate(validation_loader):
        with torch.no_grad():                               # disable gradient calculation as we will not call torch.backward()
          pred = best_model(X,length)                       # get predictions
          predict=torch.argmax(pred,dim=1).numpy()          # get predicted label
          validation_predictions=np.concatenate((validation_predictions,predict))                       # add to predictions
          validation_labels=np.concatenate((validation_labels,y))                                       # add labels to label array  
validation_accuracy=np.sum(validation_predictions == validation_labels)/len(validation_labels)          # calculate total accuracy
print('Model has accuracy on validation data: ', validation_accuracy)

# get accuracy and predictions for test set:

test_predictions=np.array([])                               # array to hold predicted labels for test set
test_labels=np.array([])                                    # array to hold actual labels for test set
for i, (X,y,length) in enumerate(test_loader):
        with torch.no_grad():                               # disable gradient calculation as we will not call torch.backward()
          pred = best_model(X,length)                       # get predictions
          predict=torch.argmax(pred,dim=1).numpy()          # get predicted label
          test_predictions=np.concatenate((test_predictions,predict))           # add to predictions
          test_labels=np.concatenate((test_labels,y))                           # add labels to label array  
test_accuracy=np.sum(test_predictions == test_labels)/len(test_labels)          # calculate total accuracy
print('Model has accuracy on test data: ', test_accuracy)

"""####Plot confusion matrices"""

from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(cm, classes,title='Confusion matrix',cmap=plt.cm.Blues):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()  
    plt.show()

cm = confusion_matrix(validation_labels, validation_predictions)
plot_confusion_matrix(cm,set(y_train),title='Confusion matrix for validation set')

cm = confusion_matrix(test_labels, test_predictions)
plot_confusion_matrix(cm,set(y_train),title='Confusion matrix for test set')